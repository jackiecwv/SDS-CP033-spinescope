# ğŸ“„ SpineScope â€“ Project Report - ğŸ”´ **Advanced Track**

Welcome to your personal project report!  
Use this file to answer the key reflection questions for each phase of the project. This report is designed to help you think like a data scientist, guide AI tools more effectively, and prepare for real-world job interviews.

---

## âœ… Week 1: Setup & Exploratory Data Analysis (EDA)

> Answer the EDA questions provided in the project materials here. Focus on data quality, trends, anomalies, and relationships.

### ğŸ”‘ Question 1: Which features are most strongly correlated with spinal abnormalities?

### ğŸ”‘ Question 2: Are any features linearly dependent on others (e.g., sacral slope and pelvic incidence)?

### ğŸ”‘ Question 3: Do biomechanical measurements cluster differently for normal vs. abnormal cases?

### ğŸ”‘ Question 4: Are there multicollinearity issues that impact modeling?

---

## âœ… Week 2: Feature Engineering & Data Preprocessing

#### ğŸ”‘ Question 1:
**Which categorical features are high-cardinality, and how will you encode them for use with embedding layers?**  

ğŸ’¡ **Hint:**  
Use `.nunique()` and `.value_counts()` to inspect cardinality.  
Use `LabelEncoder` or map categories to integer IDs.  
Think about issues like rare categories, overfitting, and embedding size selection.

âœï¸ *Your answer here...*

---

#### ğŸ”‘ Question 1:
**Which biomechanical features are likely to have the most predictive power for classifying spinal conditions, and how did you determine that?**

ğŸ’¡ **Hint:**  
Use `.groupby(target).mean()`, boxplots, or violin plots to see how each feature separates across classes.  
Use correlation matrices or feature importance from an early tree-based model as a sanity check.  
Look for features that consistently differ between classes (e.g., high pelvic tilt in abnormal cases).

âœï¸ *Your answer here...*

---

#### ğŸ”‘ Question 2:
**What numerical features in the dataset needed to be scaled before being input into a neural network, and what scaling method did you choose?**

ğŸ’¡ **Hint:**  
Use `df.describe()` and histograms to evaluate spread and skew.  
Neural networks are sensitive to feature scale.  
Choose between `StandardScaler`, `MinMaxScaler`, or log-transform based on distribution and range.

âœï¸ *Your answer here...*

---

#### ğŸ”‘ Question 3:
**Did you create any new features based on domain knowledge or feature interactions? If yes, what are they and why might they help the model better predict spinal conditions?**

ğŸ’¡ **Hint:**  
Try combining related anatomical angles or calculating ratios/differences (e.g., `pelvic_incidence - sacral_slope`).  
Think: which combinations might reflect spinal misalignment patterns?

âœï¸ *Your answer here...*

---

#### ğŸ”‘ Question 4:
**Which features, if any, did you choose to drop or simplify before modeling, and what was your justification?**

ğŸ’¡ **Hint:**  
Check for highly correlated or constant features using `.corr()` and `.nunique()`.  
Avoid overfitting by removing redundant signals.  
Be cautious about leaking target-related info if any engineered features are overly specific.

âœï¸ *Your answer here...*

---

#### ğŸ”‘ Question 5:
**After preprocessing, what does your final input schema look like (i.e., how many numerical and categorical features)? Are there class imbalance or sparsity issues to be aware of?**

ğŸ’¡ **Hint:**  
Use `.shape`, `.dtypes`, and `.value_counts()` on the target.  
Check if certain features have many near-zero or rare values.  
Look for imbalanced class distributions and think about resampling, class weights, or focal loss.

âœï¸ *Your answer here...*


---

### âœ… Week 3: Model Development & Experimentation

### ğŸ”‘ Question 1:
**What neural network architecture did you implement (input shape, number of hidden layers, activation functions, etc.), and what guided your design choices?**  
ğŸ¯ *Purpose: Tests ability to structure an FFNN for classification and explain architectural decisions.*

ğŸ’¡ **Hint:**  
Describe your model layers: e.g., `[Input â†’ Dense(64) â†’ ReLU â†’ Dropout â†’ Dense(32) â†’ ReLU â†’ Output(sigmoid)]`.  
Justify the number of layers/units based on dataset size and complexity.  
Explain why ReLU and sigmoid are appropriate.

âœï¸ *Your answer here...*

---

### ğŸ”‘ Question 2:
**What metrics did you track during training and evaluation (e.g., accuracy, precision, recall, F1-score, AUC), and how did your model perform on the validation/test set?**  
ğŸ¯ *Purpose: Tests metric understanding for classification and ability to interpret model quality.*

ğŸ’¡ **Hint:**  
Log and compare metrics across epochs using validation data.  
Plot confusion matrix and/or ROC curve.  
Explain where the model performs well and where it struggles (e.g., false positives/negatives).

âœï¸ *Your answer here...*

---

### ğŸ”‘ Question 3:
**How did the training and validation loss curves evolve during training, and what do they tell you about your model's generalization?**  
ğŸ¯ *Purpose: Tests understanding of overfitting/underfitting using learning curves.*

ğŸ’¡ **Hint:**  
Include training plots of loss and accuracy.  
Overfitting â†’ training loss drops, validation loss increases.  
Underfitting â†’ both remain high.  
Mention any regularization techniques used (dropout, early stopping).

âœï¸ *Your answer here...*

---

### ğŸ”‘ Question 4:
**How does your neural networkâ€™s performance compare to a traditional baseline (e.g., Logistic Regression or Random Forest), and what insights can you draw from this comparison?**  
ğŸ¯ *Purpose: Encourages comparative thinking and understanding model trade-offs.*

ğŸ’¡ **Hint:**  
Train a classical ML model and compare F1, AUC, and confusion matrix.  
Was the neural net better? If so, why (e.g., captured interactions)?  
If not, consider whether your DL model is under-tuned or overfitting.

âœï¸ *Your answer here...*

---

### ğŸ”‘ Question 5:
**What did you log with MLflow (e.g., model configs, metrics, training duration), and how did this help you improve your modeling workflow?**  
ğŸ¯ *Purpose: Tests reproducibility and tracking practice in a deep learning workflow.*

ğŸ’¡ **Hint:**  
Log architecture details (e.g., layer sizes, dropout rate, learning rate), metrics per epoch, and confusion matrix screenshots.  
Explain how you used logs to choose the best model or compare runs.

âœï¸ *Your answer here...*

---

## âœ… Week 4: Model Selection & Hyperparameter Tuning

### ğŸ”‘ Question 1:

### ğŸ”‘ Question 2:

### ğŸ”‘ Question 3:

### ğŸ”‘ Question 4:

### ğŸ”‘ Question 5:

---

## âœ… Week 5: Model Deployment

> Document your approach to building and deploying the Streamlit app, including design decisions, deployment steps, and challenges.

### ğŸ”‘ Question 1:

### ğŸ”‘ Question 2:

### ğŸ”‘ Question 3:

### ğŸ”‘ Question 4:

### ğŸ”‘ Question 5:

---

## âœ¨ Final Reflections

> What did you learn from this project? What would you do differently next time? What did AI tools help you with the most?

âœï¸ *Your final thoughts here...*

---
