{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0bdc6bf",
   "metadata": {},
   "source": [
    "\n",
    "# Neural Network Pipeline (PyTorch + scikit-learn + MLflow)\n",
    "\n",
    "This notebook organizes  end-to-end pipeline:\n",
    "1. Setup & Imports  \n",
    "2. Config  \n",
    "3. Model Definitions (NN, EarlyStopping)  \n",
    "4. MLflow Setup  \n",
    "5. Load Data  \n",
    "6. Preprocess / Split / Scale (+ optional SMOTE)  \n",
    "7. DataLoaders  \n",
    "8. Build Model  \n",
    "9. Train  \n",
    "10. Evaluate + Plots  \n",
    "11. Baseline Models & Comparison  \n",
    "12. Utilities (plotting helpers)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d517360",
   "metadata": {},
   "source": [
    "## 1) Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51793f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "LightGBM available: True\n",
      "CatBoost available: True\n",
      "Logistic Regression available: <class 'sklearn.linear_model._logistic.LogisticRegression'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, warnings, json, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, PowerTransformer\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             confusion_matrix, roc_auc_score, roc_curve,\n",
    "                             precision_recall_curve, average_precision_score,\n",
    "                             mean_squared_error, mean_absolute_error, r2_score)\n",
    "\n",
    "from sklearn.ensemble import (RandomForestClassifier, RandomForestRegressor,\n",
    "                              GradientBoostingClassifier, GradientBoostingRegressor)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Optional libraries\n",
    "try:\n",
    "    import lightgbm as lgb; LIGHTGBM_AVAILABLE = True\n",
    "except Exception:\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import catboost as cb; CATBOOST_AVAILABLE = True\n",
    "except Exception:\n",
    "    CATBOOST_AVAILABLE = False\n",
    "\n",
    "#logistic regression\n",
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "\n",
    "\n",
    "# MLflow\n",
    "import mlflow, mlflow.pytorch, mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Other\n",
    "from scipy import stats\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Paths / Device\n",
    "OUTPUTS_DIR = \"./outputs_nn\"\n",
    "os.makedirs(OUTPUTS_DIR, exist_ok=True)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "print(\"LightGBM available:\", LIGHTGBM_AVAILABLE)\n",
    "print(\"CatBoost available:\", CATBOOST_AVAILABLE)\n",
    "print(\"Logistic Regression available:\", lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f155e040",
   "metadata": {},
   "source": [
    "## 2) Config (hyperparameters & switches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2950c1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class Config:\n",
    "    EXPERIMENT_NAME = \"Neural_Network_Orthopedic_Classification\"\n",
    "    TRACKING_URI = \"sqlite:///mlflow_nn.db\"\n",
    "    ARTIFACT_ROOT = \"./mlruns_nn\"\n",
    "    OUTPUTS_DIR = OUTPUTS_DIR\n",
    "\n",
    "    RANDOM_STATE = 42\n",
    "    TEST_SIZE = 0.2\n",
    "    VALIDATION_SIZE = 0.2\n",
    "\n",
    "    HIDDEN_LAYERS = [128, 64, 32]\n",
    "    DROPOUT_RATE = 0.3\n",
    "    BATCH_NORM = True\n",
    "    ACTIVATION = \"relu\"\n",
    "\n",
    "    BATCH_SIZE = 32\n",
    "    MAX_EPOCHS = 200\n",
    "    LEARNING_RATE = 1e-3\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    PATIENCE = 20\n",
    "\n",
    "    DEVICE = DEVICE\n",
    "    TASK_TYPE = \"classification\"  \n",
    "    USE_SMOTE = True\n",
    "    CLASS_WEIGHTS = True\n",
    "\n",
    "cfg = Config()\n",
    "cfg.__dict__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aba0b2",
   "metadata": {},
   "source": [
    "## 3) Model Definitions (NN, EarlyStopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a6c80c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeedforwardNeuralNetwork(nn.Module):\n",
    "    \"\"\"Feedforward Neural Network with customizable architecture\"\"\"\n",
    "    def __init__(self, input_size, hidden_layers, output_size,\n",
    "                 dropout_rate=0.3, batch_norm=True, activation=\"relu\", task_type=\"classification\"):\n",
    "        super().__init__()\n",
    "        self.task_type = task_type\n",
    "        self.activation = activation\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        if activation == \"relu\":\n",
    "            act = nn.ReLU()\n",
    "        elif activation == \"tanh\":\n",
    "            act = nn.Tanh()\n",
    "        elif activation == \"sigmoid\":\n",
    "            act = nn.Sigmoid()\n",
    "        elif activation == \"leaky_relu\":\n",
    "            act = nn.LeakyReLU(0.01)\n",
    "        else:\n",
    "            act = nn.ReLU()\n",
    "\n",
    "        layers = []\n",
    "        sizes = [input_size] + hidden_layers + [output_size]\n",
    "        for i in range(len(sizes) - 1):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
    "            if i < len(sizes) - 2:\n",
    "                if batch_norm:\n",
    "                    layers.append(nn.BatchNorm1d(sizes[i+1]))\n",
    "                layers.append(act)\n",
    "                if dropout_rate > 0:\n",
    "                    layers.append(nn.Dropout(dropout_rate))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                if self.activation == \"relu\":\n",
    "                    nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                else:\n",
    "                    nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience; self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = None; self.counter = 0\n",
    "        self.best_weights = None; self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None or val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss; self.counter = 0\n",
    "            self.best_weights = model.state_dict().copy()\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                if self.restore_best_weights:\n",
    "                    model.load_state_dict(self.best_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20e7732",
   "metadata": {},
   "source": [
    "## 4) MLflow Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c5888f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/31 23:13:36 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/07/31 23:13:36 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mlflow.set_tracking_uri(cfg.TRACKING_URI)\n",
    "try:\n",
    "    exp_id = mlflow.create_experiment(cfg.EXPERIMENT_NAME, artifact_location=cfg.ARTIFACT_ROOT)\n",
    "except mlflow.exceptions.MlflowException:\n",
    "    exp = mlflow.get_experiment_by_name(cfg.EXPERIMENT_NAME)\n",
    "    exp_id = exp.experiment_id if exp else None\n",
    "mlflow.set_experiment(cfg.EXPERIMENT_NAME)\n",
    "exp_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a781cb6",
   "metadata": {},
   "source": [
    "## 5) Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a629545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(310, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pelvic_tilt</th>\n",
       "      <th>sacral_slope</th>\n",
       "      <th>lumbar_lordosis_angle</th>\n",
       "      <th>pelvic_radius</th>\n",
       "      <th>degree_spondylolisthesis</th>\n",
       "      <th>pi_ss_ratio</th>\n",
       "      <th>class</th>\n",
       "      <th>binary_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.552586</td>\n",
       "      <td>40.475232</td>\n",
       "      <td>39.609117</td>\n",
       "      <td>98.672917</td>\n",
       "      <td>-0.254400</td>\n",
       "      <td>1.557195</td>\n",
       "      <td>Hernia</td>\n",
       "      <td>Abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.060991</td>\n",
       "      <td>28.995960</td>\n",
       "      <td>25.015378</td>\n",
       "      <td>114.405425</td>\n",
       "      <td>4.564259</td>\n",
       "      <td>1.346979</td>\n",
       "      <td>Hernia</td>\n",
       "      <td>Abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.218482</td>\n",
       "      <td>46.613539</td>\n",
       "      <td>50.092194</td>\n",
       "      <td>105.985135</td>\n",
       "      <td>-3.530317</td>\n",
       "      <td>1.476653</td>\n",
       "      <td>Hernia</td>\n",
       "      <td>Abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.652878</td>\n",
       "      <td>44.644130</td>\n",
       "      <td>44.311238</td>\n",
       "      <td>101.868495</td>\n",
       "      <td>11.211523</td>\n",
       "      <td>1.552209</td>\n",
       "      <td>Hernia</td>\n",
       "      <td>Abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.652075</td>\n",
       "      <td>40.060784</td>\n",
       "      <td>28.317406</td>\n",
       "      <td>108.168725</td>\n",
       "      <td>7.918501</td>\n",
       "      <td>1.240936</td>\n",
       "      <td>Hernia</td>\n",
       "      <td>Abnormal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pelvic_tilt  sacral_slope  lumbar_lordosis_angle  pelvic_radius  \\\n",
       "0    22.552586     40.475232              39.609117      98.672917   \n",
       "1    10.060991     28.995960              25.015378     114.405425   \n",
       "2    22.218482     46.613539              50.092194     105.985135   \n",
       "3    24.652878     44.644130              44.311238     101.868495   \n",
       "4     9.652075     40.060784              28.317406     108.168725   \n",
       "\n",
       "   degree_spondylolisthesis  pi_ss_ratio   class binary_class  \n",
       "0                 -0.254400     1.557195  Hernia     Abnormal  \n",
       "1                  4.564259     1.346979  Hernia     Abnormal  \n",
       "2                 -3.530317     1.476653  Hernia     Abnormal  \n",
       "3                 11.211523     1.552209  Hernia     Abnormal  \n",
       "4                  7.918501     1.240936  Hernia     Abnormal  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Ensure the CSV is present in the working directory\n",
    "DATA_PATH = \"column_3C_processed.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e86dfd5",
   "metadata": {},
   "source": [
    "## 6) Preprocess / Split / Scale (+ optional SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5515043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((252, 6), (62, 6), (62, 6))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Optional Yeo-Johnson transform for skewed column\n",
    "if 'degree_spondylolisthesis' in df.columns:\n",
    "    pt = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "    df['degree_spondylolisthesis'] = pt.fit_transform(df[['degree_spondylolisthesis']])\n",
    "else:\n",
    "    pt = None\n",
    "\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "X = df[num_cols]\n",
    "y = df['binary_class']  # adjust if your target differs\n",
    "\n",
    "le = None\n",
    "if cfg.TASK_TYPE == \"classification\":\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "\n",
    "# Split\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=cfg.TEST_SIZE, random_state=cfg.RANDOM_STATE,\n",
    "    stratify=y if cfg.TASK_TYPE == \"classification\" else None\n",
    ")\n",
    "\n",
    "val_size_adj = cfg.VALIDATION_SIZE / (1 - cfg.TEST_SIZE)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=val_size_adj, random_state=cfg.RANDOM_STATE,\n",
    "    stratify=y_temp if cfg.TASK_TYPE == \"classification\" else None\n",
    ")\n",
    "\n",
    "# Optional SMOTE\n",
    "if cfg.TASK_TYPE == \"classification\" and cfg.USE_SMOTE:\n",
    "    sm = SMOTE(random_state=cfg.RANDOM_STATE)\n",
    "    X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_val_s   = scaler.transform(X_val)\n",
    "X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "X_train_s.shape, X_val_s.shape, X_test_s.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987c0c65",
   "metadata": {},
   "source": [
    "## 7) DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "babf6b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 2, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "to_tensor = torch.FloatTensor\n",
    "Xtr_t = to_tensor(X_train_s)\n",
    "Xva_t = to_tensor(X_val_s)\n",
    "Xte_t = to_tensor(X_test_s)\n",
    "\n",
    "if cfg.TASK_TYPE == \"classification\":\n",
    "    ytr_t = torch.LongTensor(y_train)\n",
    "    yva_t = torch.LongTensor(y_val)\n",
    "    yte_t = torch.LongTensor(y_test)\n",
    "else:\n",
    "    ytr_t = to_tensor(y_train)\n",
    "    yva_t = to_tensor(y_val)\n",
    "    yte_t = to_tensor(y_test)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(Xtr_t, ytr_t), batch_size=cfg.BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(Xva_t, yva_t), batch_size=cfg.BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(TensorDataset(Xte_t, yte_t), batch_size=cfg.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "len(train_loader), len(val_loader), len(test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c49ae1",
   "metadata": {},
   "source": [
    "## 8) Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08568fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedforwardNeuralNetwork(\n",
       "  (network): Sequential(\n",
       "    (0): Linear(in_features=6, out_features=128, bias=True)\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Dropout(p=0.3, inplace=False)\n",
       "    (8): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (9): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): Dropout(p=0.3, inplace=False)\n",
       "    (12): Linear(in_features=32, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "input_size  = X_train_s.shape[1]\n",
    "output_size = (len(np.unique(y_train)) if cfg.TASK_TYPE == \"classification\" else 1)\n",
    "\n",
    "model = FeedforwardNeuralNetwork(\n",
    "    input_size=input_size,\n",
    "    hidden_layers=cfg.HIDDEN_LAYERS,\n",
    "    output_size=output_size,\n",
    "    dropout_rate=cfg.DROPOUT_RATE,\n",
    "    batch_norm=cfg.BATCH_NORM,\n",
    "    activation=cfg.ACTIVATION,\n",
    "    task_type=cfg.TASK_TYPE\n",
    ").to(cfg.DEVICE)\n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a1fbc9",
   "metadata": {},
   "source": [
    "## 9) Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1acb1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [1. 1.]\n",
      "Epoch 000  loss 0.2701/0.3619  acc 87.70/77.42\n",
      "Epoch 010  loss 0.2576/0.3673  acc 88.89/79.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/31 23:47:07 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020  loss 0.2346/0.3780  acc 89.29/77.42\n",
      "Early stopping at epoch 20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loss\n",
    "if cfg.TASK_TYPE == \"classification\":\n",
    "    if cfg.CLASS_WEIGHTS:\n",
    "        # Compute balanced class weights from training labels\n",
    "        from sklearn.utils.class_weight import compute_class_weight\n",
    "        classes = np.unique(y_train)\n",
    "        weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "        class_weights = torch.tensor(weights, dtype=torch.float, device=cfg.DEVICE)\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        print(\"Class weights:\", weights)\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "else:\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer / Scheduler / Early stopping\n",
    "optimizer = optim.Adam(model.parameters(), lr=cfg.LEARNING_RATE, weight_decay=cfg.WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "early = EarlyStopping(patience=cfg.PATIENCE, min_delta=1e-3)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "with mlflow.start_run(run_name=\"neural_network_training\"):\n",
    "    # Log config\n",
    "    for k, v in cfg.__dict__.items():\n",
    "        if not k.startswith(\"_\"):\n",
    "            mlflow.log_param(k, v)\n",
    "\n",
    "    for epoch in range(cfg.MAX_EPOCHS):\n",
    "        model.train()\n",
    "        tr_loss, tr_correct, tr_total = 0.0, 0, 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(cfg.DEVICE), yb.to(cfg.DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb if cfg.TASK_TYPE == \"classification\" else yb.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            tr_total += yb.size(0)\n",
    "            if cfg.TASK_TYPE == \"classification\":\n",
    "                tr_correct += out.argmax(1).eq(yb).sum().item()\n",
    "\n",
    "        model.eval()\n",
    "        va_loss, va_correct, va_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(cfg.DEVICE), yb.to(cfg.DEVICE)\n",
    "                out = model(xb)\n",
    "                loss = criterion(out, yb if cfg.TASK_TYPE == \"classification\" else yb.squeeze())\n",
    "                va_loss += loss.item()\n",
    "                va_total += yb.size(0)\n",
    "                if cfg.TASK_TYPE == \"classification\":\n",
    "                    va_correct += out.argmax(1).eq(yb).sum().item()\n",
    "\n",
    "        tr_loss /= len(train_loader); va_loss /= len(val_loader)\n",
    "        train_losses.append(tr_loss); val_losses.append(va_loss)\n",
    "        scheduler.step(va_loss); early(va_loss, model)\n",
    "\n",
    "        if cfg.TASK_TYPE == \"classification\":\n",
    "            tr_acc = 100. * tr_correct / tr_total\n",
    "            va_acc = 100. * va_correct / va_total\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch:03d}  loss {tr_loss:.4f}/{va_loss:.4f}  acc {tr_acc:.2f}/{va_acc:.2f}\")\n",
    "            mlflow.log_metric(\"train_accuracy\", tr_acc, step=epoch)\n",
    "            mlflow.log_metric(\"val_accuracy\", va_acc, step=epoch)\n",
    "        else:\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch:03d}  loss {tr_loss:.4f}/{va_loss:.4f}\")\n",
    "\n",
    "        mlflow.log_metric(\"train_loss\", tr_loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_loss\", va_loss, step=epoch)\n",
    "\n",
    "        if early.early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            mlflow.log_param(\"early_stopped_epoch\", epoch)\n",
    "            break\n",
    "\n",
    "    # Save training curves\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(train_losses, label='Train')\n",
    "    plt.plot(val_losses, label='Val')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Training History'); plt.legend(); plt.grid(True)\n",
    "    os.makedirs(cfg.OUTPUTS_DIR, exist_ok=True)\n",
    "    hist_path = os.path.join(cfg.OUTPUTS_DIR, 'training_history.png')\n",
    "    plt.savefig(hist_path, dpi=300, bbox_inches='tight'); plt.close()\n",
    "    mlflow.log_artifact(hist_path)\n",
    "\n",
    "    # Log model\n",
    "    sig = infer_signature(np.asarray(X_train_s), np.asarray(y_train))\n",
    "    mlflow.pytorch.log_model(model, \"neural_network_model\", signature=sig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4414e53",
   "metadata": {},
   "source": [
    "## 10) Evaluate + Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e62aaf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN Test Metrics:\n",
      "  Accuracy: 0.8548\n",
      "  Precision: 0.8851\n",
      "  Recall: 0.8548\n",
      "  F1: 0.8591\n",
      "  ROC-AUC: 0.9548\n",
      "  Avg Precision: 0.9190\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "preds, targets, probas = [], [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(cfg.DEVICE)\n",
    "        out = model(xb)\n",
    "        if cfg.TASK_TYPE == \"classification\":\n",
    "            preds.extend(out.argmax(1).cpu().numpy())\n",
    "            probas.extend(F.softmax(out, dim=1).cpu().numpy())\n",
    "            targets.extend(yb.numpy())\n",
    "        else:\n",
    "            preds.extend(out.squeeze().cpu().numpy())\n",
    "            targets.extend(yb.numpy())\n",
    "\n",
    "if cfg.TASK_TYPE == \"classification\":\n",
    "    accuracy = accuracy_score(targets, preds)\n",
    "    precision = precision_score(targets, preds, average='weighted')\n",
    "    recall = recall_score(targets, preds, average='weighted')\n",
    "    f1 = f1_score(targets, preds, average='weighted')\n",
    "\n",
    "    if len(np.unique(targets)) == 2:\n",
    "        roc_auc = roc_auc_score(targets, np.array(probas)[:, 1])\n",
    "        avg_precision = average_precision_score(targets, np.array(probas)[:, 1])\n",
    "    else:\n",
    "        roc_auc = roc_auc_score(targets, np.array(probas), multi_class='ovr')\n",
    "        avg_precision = None\n",
    "\n",
    "    print(\"NN Test Metrics:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1: {f1:.4f}\")\n",
    "    print(f\"  ROC-AUC: {roc_auc:.4f}\")\n",
    "    if avg_precision is not None:\n",
    "        print(f\"  Avg Precision: {avg_precision:.4f}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(targets, preds)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix - NN'); plt.xlabel('Pred'); plt.ylabel('True')\n",
    "    cm_path = os.path.join(cfg.OUTPUTS_DIR, 'nn_confusion_matrix.png')\n",
    "    plt.savefig(cm_path, dpi=300, bbox_inches='tight'); plt.close()\n",
    "\n",
    "    # ROC/PR (binary)\n",
    "    if len(np.unique(targets)) == 2:\n",
    "        from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "        fpr, tpr, _ = roc_curve(targets, np.array(probas)[:, 1])\n",
    "        precision_curve, recall_curve, _ = precision_recall_curve(targets, np.array(probas)[:, 1])\n",
    "\n",
    "        plt.figure(figsize=(5,4))\n",
    "        plt.plot(fpr, tpr); plt.plot([0,1],[0,1],'k--')\n",
    "        plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title(f'ROC (AUC={roc_auc:.3f})')\n",
    "        roc_path = os.path.join(cfg.OUTPUTS_DIR, 'nn_roc.png')\n",
    "        plt.savefig(roc_path, dpi=300, bbox_inches='tight'); plt.close()\n",
    "\n",
    "        plt.figure(figsize=(5,4))\n",
    "        plt.plot(recall_curve, precision_curve)\n",
    "        plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title(f'PR (AP={avg_precision:.3f})')\n",
    "        pr_path = os.path.join(cfg.OUTPUTS_DIR, 'nn_pr.png')\n",
    "        plt.savefig(pr_path, dpi=300, bbox_inches='tight'); plt.close()\n",
    "\n",
    "    with mlflow.start_run(run_name=\"neural_network_evaluation\"):\n",
    "        for k,v in dict(accuracy=accuracy, precision=precision, recall=recall, f1=f1, roc_auc=roc_auc).items():\n",
    "            mlflow.log_metric(f\"test_{k}\", float(v))\n",
    "        if avg_precision is not None:\n",
    "            mlflow.log_metric(\"test_avg_precision\", float(avg_precision))\n",
    "        # Log artifacts\n",
    "        mlflow.log_artifact(cm_path)\n",
    "        if len(np.unique(targets)) == 2:\n",
    "            mlflow.log_artifact(roc_path); mlflow.log_artifact(pr_path)\n",
    "\n",
    "else:\n",
    "    mse = mean_squared_error(targets, preds)\n",
    "    mae = mean_absolute_error(targets, preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(targets, preds)\n",
    "    print(\"NN Test Metrics:\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  R2: {r2:.4f}\")\n",
    "\n",
    "    # Plots\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.scatter(targets, preds, alpha=0.6)\n",
    "    lims = [min(targets+preds), max(targets+preds)]\n",
    "    plt.plot(lims, lims, 'k--')\n",
    "    plt.xlabel('Actual'); plt.ylabel('Predicted'); plt.title('Pred vs Actual')\n",
    "    pva_path = os.path.join(cfg.OUTPUTS_DIR, 'nn_pred_vs_actual.png')\n",
    "    plt.savefig(pva_path, dpi=300, bbox_inches='tight'); plt.close()\n",
    "\n",
    "    residuals = np.array(targets) - np.array(preds)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.scatter(preds, residuals, alpha=0.6); plt.axhline(0, ls='--', c='k')\n",
    "    plt.xlabel('Predicted'); plt.ylabel('Residuals'); plt.title('Residuals')\n",
    "    res_path = os.path.join(cfg.OUTPUTS_DIR, 'nn_residuals.png')\n",
    "    plt.savefig(res_path, dpi=300, bbox_inches='tight'); plt.close()\n",
    "\n",
    "    with mlflow.start_run(run_name=\"neural_network_evaluation\"):\n",
    "        for k,v in dict(mse=mse, mae=mae, rmse=rmse, r2=r2).items():\n",
    "            mlflow.log_metric(f\"test_{k}\", float(v))\n",
    "        mlflow.log_artifact(pva_path); mlflow.log_artifact(res_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ff4e9c",
   "metadata": {},
   "source": [
    "## 11) Baseline Models & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98274210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Random Forest': {'accuracy': 0.8225806451612904,\n",
       "  'precision': 0.8205867256287755,\n",
       "  'recall': 0.8225806451612904,\n",
       "  'f1_score': 0.8213496813117306,\n",
       "  'roc_auc': np.float64(0.905952380952381)},\n",
       " 'Gradient Boosting': {'accuracy': 0.8225806451612904,\n",
       "  'precision': 0.8418482999128161,\n",
       "  'recall': 0.8225806451612904,\n",
       "  'f1_score': 0.8268227394401343,\n",
       "  'roc_auc': np.float64(0.8988095238095237)},\n",
       " 'LightGBM': {'accuracy': 0.8225806451612904,\n",
       "  'precision': 0.8252219849387433,\n",
       "  'recall': 0.8225806451612904,\n",
       "  'f1_score': 0.8236755045358459,\n",
       "  'roc_auc': np.float64(0.9023809523809525)},\n",
       " 'CatBoost': {'accuracy': 0.7903225806451613,\n",
       "  'precision': 0.8009494012299061,\n",
       "  'recall': 0.7903225806451613,\n",
       "  'f1_score': 0.7937539940540691,\n",
       "  'roc_auc': np.float64(0.9285714285714286)},\n",
       " 'Logistic Regression': {'accuracy': 0.8387096774193549,\n",
       "  'precision': 0.8524335031126202,\n",
       "  'recall': 0.8387096774193549,\n",
       "  'f1_score': 0.842008797653959,\n",
       "  'roc_auc': np.float64(0.9214285714285714)}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "comparison = {}\n",
    "\n",
    "def fit_and_eval_baseline(model, name):\n",
    "    model.fit(X_train_s, y_train)\n",
    "    y_pred = model.predict(X_test_s)\n",
    "\n",
    "    if cfg.TASK_TYPE == \"classification\":\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred, average='weighted')\n",
    "        rec = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        roc = None\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            proba = model.predict_proba(X_test_s)\n",
    "            if len(np.unique(y_test)) == 2:\n",
    "                roc = roc_auc_score(y_test, proba[:,1])\n",
    "            else:\n",
    "                roc = roc_auc_score(y_test, proba, multi_class='ovr')\n",
    "        comparison[name] = dict(accuracy=acc, precision=prec, recall=rec, f1_score=f1, roc_auc=roc)\n",
    "    else:\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        comparison[name] = dict(mse=mse, mae=mae, r2_score=r2, rmse=rmse)\n",
    "\n",
    "# Random Forest & Gradient Boosting\n",
    "if cfg.TASK_TYPE == \"classification\":\n",
    "    fit_and_eval_baseline(RandomForestClassifier(n_estimators=100, random_state=cfg.RANDOM_STATE), \"Random Forest\")\n",
    "    fit_and_eval_baseline(GradientBoostingClassifier(n_estimators=100, random_state=cfg.RANDOM_STATE), \"Gradient Boosting\")\n",
    "else:\n",
    "    fit_and_eval_baseline(RandomForestRegressor(n_estimators=100, random_state=cfg.RANDOM_STATE), \"Random Forest\")\n",
    "    fit_and_eval_baseline(GradientBoostingRegressor(n_estimators=100, random_state=cfg.RANDOM_STATE), \"Gradient Boosting\")\n",
    "\n",
    "# LightGBM\n",
    "if LIGHTGBM_AVAILABLE:\n",
    "    if cfg.TASK_TYPE == \"classification\":\n",
    "        fit_and_eval_baseline(lgb.LGBMClassifier(n_estimators=100, random_state=cfg.RANDOM_STATE, verbose=-1), \"LightGBM\")\n",
    "    else:\n",
    "        fit_and_eval_baseline(lgb.LGBMRegressor(n_estimators=100, random_state=cfg.RANDOM_STATE, verbose=-1), \"LightGBM\")\n",
    "\n",
    "# CatBoost\n",
    "if CATBOOST_AVAILABLE:\n",
    "    if cfg.TASK_TYPE == \"classification\":\n",
    "        fit_and_eval_baseline(cb.CatBoostClassifier(n_estimators=100, random_state=cfg.RANDOM_STATE, verbose=False), \"CatBoost\")\n",
    "    else:\n",
    "        fit_and_eval_baseline(cb.CatBoostRegressor(n_estimators=100, random_state=cfg.RANDOM_STATE, verbose=False), \"CatBoost\")\n",
    "\n",
    "#LogisticRegression\n",
    "\n",
    "\n",
    "if cfg.TASK_TYPE == \"classification\":\n",
    "    fit_and_eval_baseline(LogisticRegression(max_iter=1000, random_state=cfg.RANDOM_STATE), \"Logistic Regression\")\n",
    "else:\n",
    "    warnings.warn(\"Logistic Regression is not suitable for regression tasks.\")\n",
    "\n",
    "comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b1503e",
   "metadata": {},
   "source": [
    "### Comparison Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f76cbe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if comparison:\n",
    "    models = list(comparison.keys())\n",
    "    if cfg.TASK_TYPE == \"classification\":\n",
    "        accs = [comparison[m]['accuracy'] for m in models]\n",
    "        f1s  = [comparison[m]['f1_score'] for m in models]\n",
    "\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.bar(models, accs)\n",
    "        plt.title('Accuracy Comparison'); plt.ylabel('Accuracy'); plt.xticks(rotation=30, ha='right')\n",
    "        comp_acc_path = os.path.join(cfg.OUTPUTS_DIR, 'comparison_accuracy.png')\n",
    "        plt.savefig(comp_acc_path, dpi=300, bbox_inches='tight'); plt.close()\n",
    "\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.bar(models, f1s)\n",
    "        plt.title('F1-Score Comparison'); plt.ylabel('F1'); plt.xticks(rotation=30, ha='right')\n",
    "        comp_f1_path = os.path.join(cfg.OUTPUTS_DIR, 'comparison_f1.png')\n",
    "        plt.savefig(comp_f1_path, dpi=300, bbox_inches='tight'); plt.close()\n",
    "\n",
    "    else:\n",
    "        r2s  = [comparison[m]['r2_score'] for m in models]\n",
    "        rmses = [comparison[m]['rmse'] for m in models]\n",
    "\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.bar(models, r2s)\n",
    "        plt.title('R2 Comparison'); plt.ylabel('R2'); plt.xticks(rotation=30, ha='right')\n",
    "        comp_r2_path = os.path.join(cfg.OUTPUTS_DIR, 'comparison_r2.png')\n",
    "        plt.savefig(comp_r2_path, dpi=300, bbox_inches='tight'); plt.close()\n",
    "\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.bar(models, rmses)\n",
    "        plt.title('RMSE Comparison'); plt.ylabel('RMSE'); plt.xticks(rotation=30, ha='right')\n",
    "        comp_rmse_path = os.path.join(cfg.OUTPUTS_DIR, 'comparison_rmse.png')\n",
    "        plt.savefig(comp_rmse_path, dpi=300, bbox_inches='tight'); plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b6fe4a",
   "metadata": {},
   "source": [
    "\n",
    "## 12) Utilities\n",
    "You can add reusable plotting functions or helpers here (kept simple in this notebook for clarity).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
