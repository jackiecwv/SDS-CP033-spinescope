{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b40a486",
   "metadata": {},
   "source": [
    "\n",
    "# Classical ML Model Development Pipeline (with MLflow)\n",
    "\n",
    "This notebook is generated from your script and organized into clear, runnable sections:\n",
    "\n",
    "1. Setup & Imports  \n",
    "2. Config  \n",
    "3. Pipeline Class (methods split into logical cells)  \n",
    "4. Run Steps (load data ‚Üí split/scale ‚Üí train/tune ‚Üí compare ‚Üí advanced analysis)\n",
    "\n",
    "> Place your data file **`column_3C_processed.csv`** in the working directory before running.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6115a5",
   "metadata": {},
   "source": [
    "## 1) Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a7d428f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, \n",
    "    RandomizedSearchCV, StratifiedKFold\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix, roc_auc_score, roc_curve,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.inspection import permutation_importance\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pickle\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import shap\n",
    "from sklearn.model_selection import learning_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from scipy import stats\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# --- ENSURE OUTPUT DIRECTORY EXISTS ---\n",
    "os.makedirs('outputs_ml', exist_ok=True)\n",
    "\n",
    "print(\"Imports complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752098e7",
   "metadata": {},
   "source": [
    "## 2) Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d962736f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration class for model development pipeline\"\"\"\n",
    "    EXPERIMENT_NAME = \"Orthopedic_Patients_Classification\"\n",
    "    MODEL_REGISTRY_NAME = \"orthopedic_classifier\"\n",
    "    RANDOM_STATE = 42\n",
    "    TEST_SIZE = 0.2\n",
    "    VALIDATION_SIZE = 0.2  # From training set\n",
    "    CV_FOLDS = 5\n",
    "    MAX_EVALS = 50  # For hyperparameter tuning\n",
    "    \n",
    "    # Class imbalance handling\n",
    "    IMBALANCE_STRATEGY = \"SMOTE\"  # Options: \"SMOTE\", \"UNDERSAMPLING\", \"SMOTEENN\", \"WEIGHTED\"\n",
    "    \n",
    "    # Linear model preprocessing\n",
    "    POWER_TRANSFORM = True  # Apply Yeo-Johnson transformation\n",
    "    VIF_THRESHOLD = 5.0  # Variance Inflation Factor threshold\n",
    "    OUTLIER_REMOVAL = True  # Remove outliers for linear models\n",
    "    \n",
    "    # MLflow tracking\n",
    "    TRACKING_URI = \"sqlite:///mlflow.db\"  # Use SQLite for local tracking\n",
    "    ARTIFACT_ROOT = \"./mlruns\"\n",
    "\n",
    "cfg = Config()\n",
    "cfg.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b547a264",
   "metadata": {},
   "source": [
    "## 3) Pipeline Class ‚Äî Initialization & MLflow Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6352a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelDevelopmentPipeline:\n",
    "    \"\"\"Comprehensive model development pipeline with MLflow tracking\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.df = None\n",
    "        self.X_train = None\n",
    "        self.X_val = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_val = None\n",
    "        self.y_test = None\n",
    "        self.scaler = None\n",
    "        self.standard_scaler = None\n",
    "        self.power_transformer = None\n",
    "        self.label_encoder = None\n",
    "        self.feature_names = None\n",
    "        self.selected_features = None\n",
    "        self.models_performance = {}\n",
    "        self.smote = None\n",
    "        self.outlier_mask = None\n",
    "        \n",
    "        # Setup MLflow\n",
    "        self.setup_mlflow()\n",
    "    \n",
    "    def setup_mlflow(self):\n",
    "        \"\"\"Initialize MLflow tracking\"\"\"\n",
    "        mlflow.set_tracking_uri(self.config.TRACKING_URI)\n",
    "        \n",
    "        # Create experiment if it doesn't exist\n",
    "        try:\n",
    "            experiment_id = mlflow.create_experiment(\n",
    "                name=self.config.EXPERIMENT_NAME,\n",
    "                artifact_location=self.config.ARTIFACT_ROOT\n",
    "            )\n",
    "        except mlflow.exceptions.MlflowException:\n",
    "            experiment = mlflow.get_experiment_by_name(self.config.EXPERIMENT_NAME)\n",
    "            experiment_id = experiment.experiment_id if experiment else None\n",
    "        \n",
    "        mlflow.set_experiment(self.config.EXPERIMENT_NAME)\n",
    "        print(f\"MLflow experiment: {self.config.EXPERIMENT_NAME}\")\n",
    "        print(f\"Experiment ID: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a043a883",
   "metadata": {},
   "source": [
    "### 3.1 Load & Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cc086bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _mdp_load_and_prepare_data(self):\n",
    "    \"\"\"Load and prepare the dataset\"\"\"\n",
    "    print(\"Loading and preparing data...\")\n",
    "    self.df = pd.read_csv('column_3C_processed.csv')\n",
    "    print(f\"Dataset shape: {self.df.shape}\")\n",
    "    print(f\"Target distribution:\\n{self.df['binary_class'].value_counts()}\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"data_preparation\"):\n",
    "        mlflow.log_param(\"dataset_shape\", self.df.shape)\n",
    "        mlflow.log_param(\"n_features\", len(self.df.select_dtypes(include=[np.number]).columns))\n",
    "        mlflow.log_param(\"target_classes\", list(self.df['binary_class'].unique()))\n",
    "        mlflow.log_param(\"class_distribution\", dict(self.df['binary_class'].value_counts()))\n",
    "        mlflow.log_metric(\"missing_values\", self.df.isnull().sum().sum())\n",
    "        mlflow.log_metric(\"duplicate_rows\", self.df.duplicated().sum())\n",
    "        \n",
    "        dataset_info = {\n",
    "            \"shape\": self.df.shape,\n",
    "            \"columns\": list(self.df.columns),\n",
    "            \"dtypes\": {col: str(dtype) for col, dtype in self.df.dtypes.items()},\n",
    "            \"missing_values\": self.df.isnull().sum().to_dict(),\n",
    "            \"class_distribution\": self.df['binary_class'].value_counts().to_dict()\n",
    "        }\n",
    "        with open(\"outputs_ml/dataset_info.json\", \"w\") as f:\n",
    "            json.dump(dataset_info, f, indent=2)\n",
    "        mlflow.log_artifact(\"outputs_ml/dataset_info.json\")\n",
    "        os.remove(\"outputs_ml/dataset_info.json\")\n",
    "ModelDevelopmentPipeline.load_and_prepare_data = _mdp_load_and_prepare_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3a446b",
   "metadata": {},
   "source": [
    "### 3.2 Check Linear Model Assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f5a8499",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _mdp_check_linear_model_assumptions(self, X, y):\n",
    "    print(\"Checking linear model assumptions...\")\n",
    "    assumptions_results = {}\n",
    "    # VIF\n",
    "    print(\"  Checking multicollinearity...\")\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    high_vif_features = vif_data[vif_data[\"VIF\"] > self.config.VIF_THRESHOLD][\"Feature\"].tolist()\n",
    "    assumptions_results[\"high_vif_features\"] = high_vif_features\n",
    "    assumptions_results[\"vif_data\"] = vif_data\n",
    "    print(f\"    Features with VIF > {self.config.VIF_THRESHOLD}: {len(high_vif_features)}\")\n",
    "    # Normality\n",
    "    print(\"  Checking feature normality...\")\n",
    "    normality_results = {}\n",
    "    for col in X.columns:\n",
    "        stat, p_value = stats.shapiro(X[col])\n",
    "        normality_results[col] = {\"statistic\": float(stat), \"p_value\": float(p_value), \"is_normal\": bool(p_value > 0.05)}\n",
    "    non_normal_features = [col for col, r in normality_results.items() if not r[\"is_normal\"]]\n",
    "    assumptions_results[\"non_normal_features\"] = non_normal_features\n",
    "    assumptions_results[\"normality_results\"] = normality_results\n",
    "    print(f\"    Non-normal features: {len(non_normal_features)}\")\n",
    "    # Outliers\n",
    "    print(\"  Checking for outliers...\")\n",
    "    outlier_counts = {}\n",
    "    for col in X.columns:\n",
    "        Q1, Q3 = X[col].quantile(0.25), X[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lb, ub = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n",
    "        outliers = ((X[col] < lb) | (X[col] > ub)).sum()\n",
    "        outlier_counts[col] = int(outliers)\n",
    "    assumptions_results[\"outlier_counts\"] = outlier_counts\n",
    "    total_outliers = sum(outlier_counts.values())\n",
    "    print(f\"    Total outliers detected: {total_outliers}\")\n",
    "    # Save summaries\n",
    "    vif_data.to_csv(\"outputs_ml/vif_data.csv\", index=False)\n",
    "    with open(\"outputs_ml/normality_results.json\", \"w\") as f: json.dump(normality_results, f, indent=2)\n",
    "    with open(\"outputs_ml/outlier_counts.json\", \"w\") as f: json.dump(outlier_counts, f, indent=2)\n",
    "    return assumptions_results\n",
    "ModelDevelopmentPipeline.check_linear_model_assumptions = _mdp_check_linear_model_assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc762cc",
   "metadata": {},
   "source": [
    "### 3.3 Apply Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94dd1b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _mdp_apply_transformations(self, X_train, X_val, X_test, for_linear_models=False):\n",
    "    print(\"Applying data transformations...\")\n",
    "    X_train_t = X_train.copy(); X_val_t = X_val.copy(); X_test_t = X_test.copy()\n",
    "    if 'degree_spondylolisthesis' in X_train_t.columns:\n",
    "        print(\"  Applying power transformation to degree_spondylolisthesis...\")\n",
    "        self.degree_power_transformer = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "        X_train_t['degree_spondylolisthesis'] = self.degree_power_transformer.fit_transform(X_train_t[['degree_spondylolisthesis']]).flatten()\n",
    "        X_val_t['degree_spondylolisthesis'] = self.degree_power_transformer.transform(X_val_t[['degree_spondylolisthesis']]).flatten()\n",
    "        X_test_t['degree_spondylolisthesis'] = self.degree_power_transformer.transform(X_test_t[['degree_spondylolisthesis']]).flatten()\n",
    "        print(f\"    Power transformation applied (lambda: {self.degree_power_transformer.lambdas_[0]:.4f})\")\n",
    "    else:\n",
    "        print(\"  Warning: degree_spondylolisthesis column not found in dataset.\")\n",
    "    if for_linear_models:\n",
    "        print(\"  Applying standard scaling to all columns...\")\n",
    "        self.standard_scaler = StandardScaler()\n",
    "        X_train_t = pd.DataFrame(self.standard_scaler.fit_transform(X_train_t), columns=X_train_t.columns, index=X_train_t.index)\n",
    "        X_val_t   = pd.DataFrame(self.standard_scaler.transform(X_val_t), columns=X_val_t.columns, index=X_val_t.index)\n",
    "        X_test_t  = pd.DataFrame(self.standard_scaler.transform(X_test_t), columns=X_test_t.columns, index=X_test_t.index)\n",
    "        print(\"  Applying feature selection...\")\n",
    "        k_best = min(len(X_train_t.columns) - 1, 4)\n",
    "        selector = SelectKBest(score_func=f_classif, k=k_best)\n",
    "        X_train_t = selector.fit_transform(X_train_t, self.y_train)\n",
    "        X_val_t   = selector.transform(X_val_t)\n",
    "        X_test_t  = selector.transform(X_test_t)\n",
    "        self.selected_features = [self.feature_names[i] for i in selector.get_support(indices=True)]\n",
    "        print(f\"    Selected features: {self.selected_features}\")\n",
    "    else:\n",
    "        print(\"  Applying standard scaling to all columns...\")\n",
    "        self.scaler = StandardScaler()\n",
    "        X_train_t = self.scaler.fit_transform(X_train_t)\n",
    "        X_val_t   = self.scaler.transform(X_val_t)\n",
    "        X_test_t  = self.scaler.transform(X_test_t)\n",
    "    return X_train_t, X_val_t, X_test_t\n",
    "ModelDevelopmentPipeline.apply_transformations = _mdp_apply_transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c15233",
   "metadata": {},
   "source": [
    "### 3.4 Handle Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2f5f20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _mdp_handle_class_imbalance(self, X_train, y_train):\n",
    "    print(f\"Handling class imbalance using {self.config.IMBALANCE_STRATEGY}...\")\n",
    "    print(\"  Original distribution:\", dict(pd.Series(y_train).value_counts().sort_index()))\n",
    "    X_res, y_res = X_train, y_train\n",
    "    if self.config.IMBALANCE_STRATEGY == \"SMOTE\":\n",
    "        self.smote = SMOTE(random_state=self.config.RANDOM_STATE)\n",
    "        X_res, y_res = self.smote.fit_resample(X_train, y_train)\n",
    "    elif self.config.IMBALANCE_STRATEGY == \"UNDERSAMPLING\":\n",
    "        rus = RandomUnderSampler(random_state=self.config.RANDOM_STATE)\n",
    "        X_res, y_res = rus.fit_resample(X_train, y_train)\n",
    "    elif self.config.IMBALANCE_STRATEGY == \"SMOTEENN\":\n",
    "        smote_enn = SMOTEENN(random_state=self.config.RANDOM_STATE)\n",
    "        X_res, y_res = smote_enn.fit_resample(X_train, y_train)\n",
    "    elif self.config.IMBALANCE_STRATEGY == \"WEIGHTED\":\n",
    "        print(\"  Using class weights in models...\")\n",
    "        return X_train, y_train\n",
    "    print(\"  New distribution:\", dict(pd.Series(y_res).value_counts().sort_index()))\n",
    "    return X_res, y_res\n",
    "ModelDevelopmentPipeline.handle_class_imbalance = _mdp_handle_class_imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d8143f",
   "metadata": {},
   "source": [
    "### 3.5 Split & Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3b0b6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _mdp_split_and_scale_data(self, target_col='binary_class'):\n",
    "    print(\"Splitting and scaling data...\")\n",
    "    numerical_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    X = self.df[numerical_cols]; y = self.df[target_col]\n",
    "    self.label_encoder = LabelEncoder(); y_enc = self.label_encoder.fit_transform(y)\n",
    "    self.feature_names = numerical_cols\n",
    "    X_temp, self.X_test, y_temp, self.y_test = train_test_split(\n",
    "        X, y_enc, test_size=self.config.TEST_SIZE, random_state=self.config.RANDOM_STATE, stratify=y_enc\n",
    "    )\n",
    "    val_size_adj = self.config.VALIDATION_SIZE / (1 - self.config.TEST_SIZE)\n",
    "    self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_size_adj, random_state=self.config.RANDOM_STATE, stratify=y_temp\n",
    "    )\n",
    "    print(f\"Training set: {self.X_train.shape}\")\n",
    "    print(f\"Validation set: {self.X_val.shape}\")\n",
    "    print(f\"Test set: {self.X_test.shape}\")\n",
    "    assumptions = self.check_linear_model_assumptions(self.X_train, self.y_train)\n",
    "    self.X_train_linear, self.X_val_linear, self.X_test_linear = self.apply_transformations(self.X_train, self.X_val, self.X_test, for_linear_models=True)\n",
    "    self.X_train_scaled, self.X_val_scaled, self.X_test_scaled = self.apply_transformations(self.X_train, self.X_val, self.X_test, for_linear_models=False)\n",
    "    self.X_train_balanced, self.y_train_balanced = self.handle_class_imbalance(self.X_train_scaled, self.y_train)\n",
    "    self.X_train_linear_balanced, self.y_train_linear_balanced = self.handle_class_imbalance(self.X_train_linear, self.y_train)\n",
    "    with mlflow.start_run(run_name=\"data_splitting\"):\n",
    "        mlflow.log_param(\"train_size\", len(self.X_train))\n",
    "        mlflow.log_param(\"val_size\", len(self.X_val))\n",
    "        mlflow.log_param(\"test_size\", len(self.X_test))\n",
    "        mlflow.log_param(\"n_features\", len(self.feature_names))\n",
    "        mlflow.log_param(\"feature_names\", self.feature_names)\n",
    "        mlflow.log_param(\"target_encoding\", dict(zip(self.label_encoder.classes_, self.label_encoder.transform(self.label_encoder.classes_))))\n",
    "        mlflow.log_param(\"imbalance_strategy\", self.config.IMBALANCE_STRATEGY)\n",
    "        mlflow.log_param(\"power_transform\", self.config.POWER_TRANSFORM)\n",
    "        mlflow.log_param(\"vif_threshold\", self.config.VIF_THRESHOLD)\n",
    "        mlflow.log_param(\"high_vif_features\", assumptions[\"high_vif_features\"])\n",
    "        mlflow.log_param(\"non_normal_features\", assumptions[\"non_normal_features\"])\n",
    "        mlflow.log_metric(\"total_outliers\", sum(assumptions[\"outlier_counts\"].values()))\n",
    "        mlflow.log_metric(\"balanced_train_size\", len(self.X_train_balanced))\n",
    "        if self.selected_features:\n",
    "            mlflow.log_param(\"selected_features\", self.selected_features)\n",
    "ModelDevelopmentPipeline.split_and_scale_data = _mdp_split_and_scale_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1889311",
   "metadata": {},
   "source": [
    "### 3.6 Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b2f7ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _mdp_get_model_configurations(self) -> Dict[str, Dict]:\n",
    "    use_class_weights = self.config.IMBALANCE_STRATEGY == \"WEIGHTED\"\n",
    "    return {\n",
    "        'logistic_regression': {\n",
    "            'model': LogisticRegression(random_state=self.config.RANDOM_STATE, max_iter=2000),\n",
    "            'params': {\n",
    "                'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "                'solver': ['liblinear', 'saga'],\n",
    "                'class_weight': ['balanced'] if use_class_weights else [None, 'balanced']\n",
    "            },\n",
    "            'is_linear': True\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'model': RandomForestClassifier(random_state=self.config.RANDOM_STATE),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [None, 10, 20, 30],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "                'max_features': ['sqrt', 'log2', None],\n",
    "                'class_weight': ['balanced'] if use_class_weights else [None, 'balanced']\n",
    "            },\n",
    "            'is_linear': False\n",
    "        },\n",
    "        'gradient_boosting': {\n",
    "            'model': GradientBoostingClassifier(random_state=self.config.RANDOM_STATE),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "                'subsample': [0.8, 0.9, 1.0]\n",
    "            },\n",
    "            'is_linear': False\n",
    "        },\n",
    "        'svm': {\n",
    "            'model': SVC(random_state=self.config.RANDOM_STATE, probability=True),\n",
    "            'params': {\n",
    "                'C': [0.1, 1, 10, 100],\n",
    "                'kernel': ['rbf', 'poly', 'linear'],\n",
    "                'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "                'class_weight': ['balanced'] if use_class_weights else [None, 'balanced']\n",
    "            },\n",
    "            'is_linear': True\n",
    "        },\n",
    "        'naive_bayes': {\n",
    "            'model': GaussianNB(),\n",
    "            'params': {\n",
    "                'var_smoothing': np.logspace(-10, -6, 10)\n",
    "            },\n",
    "            'is_linear': True\n",
    "        },\n",
    "        'decision_tree': {\n",
    "            'model': DecisionTreeClassifier(random_state=self.config.RANDOM_STATE),\n",
    "            'params': {\n",
    "                'max_depth': [None, 5, 10, 15, 20],\n",
    "                'min_samples_split': [2, 5, 10, 20],\n",
    "                'min_samples_leaf': [1, 2, 5, 10],\n",
    "                'max_features': ['sqrt', 'log2', None],\n",
    "                'class_weight': ['balanced'] if use_class_weights else [None, 'balanced']\n",
    "            },\n",
    "            'is_linear': False\n",
    "        },\n",
    "        'knn': {\n",
    "            'model': KNeighborsClassifier(),\n",
    "            'params': {\n",
    "                'n_neighbors': [3, 5, 7, 9, 11, 15],\n",
    "                'weights': ['uniform', 'distance'],\n",
    "                'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "            },\n",
    "            'is_linear': False\n",
    "        },\n",
    "        'mlp': {\n",
    "            'model': MLPClassifier(random_state=self.config.RANDOM_STATE, max_iter=2000),\n",
    "            'params': {\n",
    "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "                'activation': ['relu', 'tanh'],\n",
    "                'solver': ['adam', 'lbfgs'],\n",
    "                'alpha': [0.0001, 0.001, 0.01],\n",
    "                'learning_rate': ['constant', 'adaptive']\n",
    "            },\n",
    "            'is_linear': False\n",
    "        }\n",
    "    }\n",
    "ModelDevelopmentPipeline.get_model_configurations = _mdp_get_model_configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8b4a91",
   "metadata": {},
   "source": [
    "### 3.7 Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "385b2329",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _mdp_evaluate_model(self, model, X_test, y_test, model_name: str) -> Dict[str, float]:\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "        'recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "        'f1_score': f1_score(y_test, y_pred, average='weighted'),\n",
    "        'precision_macro': precision_score(y_test, y_pred, average='macro'),\n",
    "        'recall_macro': recall_score(y_test, y_pred, average='macro'),\n",
    "        'f1_macro': f1_score(y_test, y_pred, average='macro'),\n",
    "    }\n",
    "    if y_prob is not None:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_test, y_prob)\n",
    "        metrics['avg_precision'] = average_precision_score(y_test, y_prob)\n",
    "    cv_scores = cross_val_score(model, X_test, y_test, cv=self.config.CV_FOLDS, scoring='accuracy')\n",
    "    metrics['cv_accuracy_mean'] = cv_scores.mean()\n",
    "    metrics['cv_accuracy_std'] = cv_scores.std()\n",
    "    return metrics\n",
    "ModelDevelopmentPipeline.evaluate_model = _mdp_evaluate_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac36655",
   "metadata": {},
   "source": [
    "### 3.8 Create Evaluation Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3630124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _mdp_create_evaluation_plots(self, model, X_test, y_test, model_name: str, feature_names: List[str]):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])\n",
    "    axes[0, 0].set_title(f'Confusion Matrix - {model_name}')\n",
    "    axes[0, 0].set_ylabel('True Label')\n",
    "    axes[0, 0].set_xlabel('Predicted Label')\n",
    "    if y_prob is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        auc_score = roc_auc_score(y_test, y_prob)\n",
    "        axes[0, 1].plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.3f})')\n",
    "        axes[0, 1].plot([0, 1], [0, 1], 'k--')\n",
    "        axes[0, 1].set_xlabel('False Positive Rate')\n",
    "        axes[0, 1].set_ylabel('True Positive Rate')\n",
    "        axes[0, 1].set_title(f'ROC Curve - {model_name}')\n",
    "        axes[0, 1].legend()\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "        avg_precision = average_precision_score(y_test, y_prob)\n",
    "        axes[1, 0].plot(recall, precision, label=f'PR Curve (AP = {avg_precision:.3f})')\n",
    "        axes[1, 0].set_xlabel('Recall')\n",
    "        axes[1, 0].set_ylabel('Precision')\n",
    "        axes[1, 0].set_title(f'Precision-Recall Curve - {model_name}')\n",
    "        axes[1, 0].legend()\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_importance = pd.DataFrame({'feature': feature_names,'importance': model.feature_importances_}).sort_values('importance', ascending=True)\n",
    "        axes[1, 1].barh(feature_importance['feature'], feature_importance['importance'])\n",
    "        axes[1, 1].set_title(f'Feature Importance - {model_name}'); axes[1, 1].set_xlabel('Importance')\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        coef_importance = pd.DataFrame({'feature': feature_names,'coefficient': np.abs(model.coef_[0])}).sort_values('coefficient', ascending=True)\n",
    "        axes[1, 1].barh(coef_importance['feature'], coef_importance['coefficient'])\n",
    "        axes[1, 1].set_title(f'Feature Coefficients - {model_name}'); axes[1, 1].set_xlabel('Absolute Coefficient')\n",
    "    plt.tight_layout()\n",
    "    out_path = f'outputs_ml/{model_name}_evaluation.png'\n",
    "    plt.savefig(out_path, dpi=300, bbox_inches='tight'); plt.close()\n",
    "    return out_path\n",
    "ModelDevelopmentPipeline.create_evaluation_plots = _mdp_create_evaluation_plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d22838",
   "metadata": {},
   "source": [
    "### 3.9 Train & Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3298830",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _mdp_train_and_evaluate_models(self):\n",
    "    print(\"Training and evaluating models...\")\n",
    "    model_configs = self.get_model_configurations()\n",
    "    for model_name, config in model_configs.items():\n",
    "        print(f\"\\n{'='*50}\\nTraining {model_name.upper()}\\n{'='*50}\")\n",
    "        with mlflow.start_run(run_name=f\"{model_name}_training\"):\n",
    "            try:\n",
    "                mlflow.log_param(\"model_type\", model_name)\n",
    "                mlflow.log_param(\"random_state\", self.config.RANDOM_STATE)\n",
    "                mlflow.log_param(\"is_linear_model\", config.get('is_linear', False))\n",
    "                is_linear = config.get('is_linear', False)\n",
    "                if is_linear:\n",
    "                    if self.config.IMBALANCE_STRATEGY == \"WEIGHTED\":\n",
    "                        X_train_use, y_train_use = self.X_train_linear, self.y_train\n",
    "                    else:\n",
    "                        X_train_use, y_train_use = self.X_train_linear_balanced, self.y_train_linear_balanced\n",
    "                    X_val_use, X_test_use = self.X_val_linear, self.X_test_linear\n",
    "                    feature_names = self.selected_features if self.selected_features else self.feature_names\n",
    "                else:\n",
    "                    if self.config.IMBALANCE_STRATEGY == \"WEIGHTED\":\n",
    "                        X_train_use, y_train_use = self.X_train_scaled, self.y_train\n",
    "                    else:\n",
    "                        X_train_use, y_train_use = self.X_train_balanced, self.y_train_balanced\n",
    "                    X_val_use, X_test_use = self.X_val_scaled, self.X_test_scaled\n",
    "                    feature_names = self.feature_names\n",
    "                mlflow.log_param(\"data_preprocessing\", \"linear_transformed\" if is_linear else \"standard_scaled\")\n",
    "                mlflow.log_param(\"train_samples\", len(X_train_use))\n",
    "                mlflow.log_param(\"features_used\", feature_names)\n",
    "                print(\"Performing hyperparameter tuning...\")\n",
    "                search = RandomizedSearchCV(\n",
    "                    estimator=config['model'],\n",
    "                    param_distributions=config['params'],\n",
    "                    n_iter=min(self.config.MAX_EVALS, np.prod([len(v) if isinstance(v, list) else 1 for v in config['params'].values()])), \n",
    "                    cv=StratifiedKFold(n_splits=self.config.CV_FOLDS, shuffle=True, random_state=self.config.RANDOM_STATE),\n",
    "                    scoring='f1_weighted', n_jobs=-1, random_state=self.config.RANDOM_STATE, verbose=1\n",
    "                )\n",
    "                search.fit(X_train_use, y_train_use)\n",
    "                best_model = search.best_estimator_\n",
    "                mlflow.log_params(search.best_params_); mlflow.log_metric(\"best_cv_score\", search.best_score_)\n",
    "                val_metrics = self.evaluate_model(best_model, X_val_use, self.y_val, model_name)\n",
    "                for k,v in val_metrics.items(): mlflow.log_metric(f\"val_{k}\", v)\n",
    "                test_metrics = self.evaluate_model(best_model, X_test_use, self.y_test, model_name)\n",
    "                for k,v in test_metrics.items(): mlflow.log_metric(f\"test_{k}\", v)\n",
    "                self.models_performance[model_name] = {\n",
    "                    'model': best_model, 'best_params': search.best_params_,\n",
    "                    'val_metrics': val_metrics, 'test_metrics': test_metrics,\n",
    "                    'is_linear': is_linear, 'X_test_use': X_test_use, 'feature_names': feature_names\n",
    "                }\n",
    "                plot_path = self.create_evaluation_plots(best_model, X_test_use, self.y_test, model_name, feature_names)\n",
    "                mlflow.log_artifact(plot_path); os.remove(plot_path)\n",
    "                signature = infer_signature(X_train_use, y_train_use)\n",
    "                mlflow.sklearn.log_model(sk_model=best_model, artifact_path=f\"model_{model_name}\",\n",
    "                                         signature=signature, input_example=X_train_use[:5])\n",
    "                if hasattr(best_model, 'feature_importances_'):\n",
    "                    fi = pd.DataFrame({'feature': feature_names, 'importance': best_model.feature_importances_}).sort_values('importance', ascending=False)\n",
    "                    top_features = fi.head(5)['feature'].tolist()\n",
    "                    mlflow.log_param(\"top_5_features\", top_features)\n",
    "                    fi_path = f'outputs_ml/{model_name}_feature_importance.csv'\n",
    "                    fi.to_csv(fi_path, index=False); mlflow.log_artifact(fi_path); os.remove(fi_path)\n",
    "                print(f\"‚úì {model_name} training completed\")\n",
    "                print(f\"  Best validation F1: {val_metrics['f1_score']:.4f}\")\n",
    "                print(f\"  Test F1: {test_metrics['f1_score']:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚úó Error training {model_name}: {str(e)}\")\n",
    "                mlflow.log_param(\"error\", str(e))\n",
    "                continue\n",
    "ModelDevelopmentPipeline.train_and_evaluate_models = _mdp_train_and_evaluate_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d2733",
   "metadata": {},
   "source": [
    "### 3.10 Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ee9df52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _mdp_compare_models(self):\n",
    "    print(\"\\n\" + \"=\"*60); print(\"MODEL COMPARISON AND SELECTION\"); print(\"=\"*60)\n",
    "    with mlflow.start_run(run_name=\"model_comparison\"):\n",
    "        comparison_data = []\n",
    "        for model_name, perf in self.models_performance.items():\n",
    "            row = {\n",
    "                'Model': model_name,\n",
    "                'Val_Accuracy': perf['val_metrics']['accuracy'],\n",
    "                'Val_Precision': perf['val_metrics']['precision'],\n",
    "                'Val_Recall': perf['val_metrics']['recall'],\n",
    "                'Val_F1': perf['val_metrics']['f1_score'],\n",
    "                'Test_Accuracy': perf['test_metrics']['accuracy'],\n",
    "                'Test_Precision': perf['test_metrics']['precision'],\n",
    "                'Test_Recall': perf['test_metrics']['recall'],\n",
    "                'Test_F1': perf['test_metrics']['f1_score'],\n",
    "            }\n",
    "            if 'roc_auc' in perf['test_metrics']:\n",
    "                row['Test_ROC_AUC'] = perf['test_metrics']['roc_auc']\n",
    "            comparison_data.append(row)\n",
    "        comparison_df = pd.DataFrame(comparison_data).sort_values('Test_F1', ascending=False)\n",
    "        print(\"Model Performance Comparison:\"); print(comparison_df.round(4))\n",
    "        comp_csv_path = 'outputs_ml/model_comparison.csv'; comparison_df.to_csv(comp_csv_path, index=False); mlflow.log_artifact(comp_csv_path)\n",
    "        best_model_name = comparison_df.iloc[0]['Model']; best_model_f1 = comparison_df.iloc[0]['Test_F1']\n",
    "        mlflow.log_param(\"best_model\", best_model_name); mlflow.log_metric(\"best_model_f1\", best_model_f1)\n",
    "        print(f\"\\nüèÜ Best Model: {best_model_name} (Test F1: {best_model_f1:.4f})\")\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        metrics_to_plot = ['Test_Accuracy', 'Test_Precision', 'Test_Recall', 'Test_F1']\n",
    "        for i, metric in enumerate(metrics_to_plot):\n",
    "            ax = axes[i//2, i%2]; bars = ax.bar(comparison_df['Model'], comparison_df[metric])\n",
    "            ax.set_title(f'{metric.replace(\"_\", \" \")} Comparison'); ax.set_ylabel(metric.replace(\"_\", \" \")); ax.tick_params(axis='x', rotation=45)\n",
    "            best_idx = comparison_df[metric].idxmax(); bars[list(comparison_df.index).index(best_idx)].set_color('gold')\n",
    "            for j, v in enumerate(comparison_df[metric]): ax.text(j, v + 0.005, f'{v:.3f}', ha='center', va='bottom')\n",
    "        plt.tight_layout(); comp_png_path = 'outputs_ml/model_comparison_chart.png'\n",
    "        plt.savefig(comp_png_path, dpi=300, bbox_inches='tight'); mlflow.log_artifact(comp_png_path); plt.close()\n",
    "        os.remove(comp_csv_path); os.remove(comp_png_path)\n",
    "        return best_model_name\n",
    "ModelDevelopmentPipeline.compare_models = _mdp_compare_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fde3234",
   "metadata": {},
   "source": [
    "### 3.11 Advanced Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeb65343",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _mdp_advanced_model_analysis(self, best_model_name: str):\n",
    "    print(\"\\n\" + \"=\"*60); print(f\"ADVANCED ANALYSIS - {best_model_name.upper()}\"); print(\"=\"*60)\n",
    "    best = self.models_performance[best_model_name]\n",
    "    best_model = best['model']; is_linear = best['is_linear']; X_test_use = best['X_test_use']; feature_names = best['feature_names']\n",
    "    with mlflow.start_run(run_name=f\"{best_model_name}_advanced_analysis\"):\n",
    "        print(\"Generating learning curves...\")\n",
    "        train_sizes, train_scores, val_scores = learning_curve(best_model, X_test_use, self.y_test, cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(train_sizes, train_scores.mean(axis=1), 'o-', label='Training Score')\n",
    "        plt.plot(train_sizes, val_scores.mean(axis=1), 'o-', label='Validation Score')\n",
    "        plt.fill_between(train_sizes, train_scores.mean(axis=1) - train_scores.std(axis=1),\n",
    "                         train_scores.mean(axis=1) + train_scores.std(axis=1), alpha=0.1)\n",
    "        plt.fill_between(train_sizes, val_scores.mean(axis=1) - val_scores.std(axis=1),\n",
    "                         val_scores.mean(axis=1) + val_scores.std(axis=1), alpha=0.1)\n",
    "        plt.xlabel('Training Set Size'); plt.ylabel('Score'); plt.title(f'Learning Curves - {best_model_name}'); plt.legend(); plt.grid(True, alpha=0.3)\n",
    "        lc_path = 'outputs_ml/learning_curves.png'; plt.savefig(lc_path, dpi=300, bbox_inches='tight'); mlflow.log_artifact(lc_path); plt.close()\n",
    "        print(\"Calculating permutation importance...\")\n",
    "        perm = permutation_importance(best_model, X_test_use, self.y_test, n_repeats=10, random_state=self.config.RANDOM_STATE)\n",
    "        perm_df = pd.DataFrame({'feature': feature_names, 'importance_mean': perm.importances_mean, 'importance_std': perm.importances_std}).sort_values('importance_mean', ascending=False)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(perm_df['feature'], perm_df['importance_mean'], xerr=perm_df['importance_std'])\n",
    "        plt.xlabel('Permutation Importance'); plt.title(f'Permutation Importance - {best_model_name}'); plt.tight_layout()\n",
    "        perm_path = 'outputs_ml/permutation_importance.png'; plt.savefig(perm_path, dpi=300, bbox_inches='tight'); mlflow.log_artifact(perm_path); plt.close()\n",
    "        print(\"Analyzing model calibration...\")\n",
    "        if hasattr(best_model, 'predict_proba'):\n",
    "            from sklearn.calibration import calibration_curve\n",
    "            y_prob = best_model.predict_proba(X_test_use)[:, 1]\n",
    "            fop, mpv = calibration_curve(self.y_test, y_prob, n_bins=10)\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(mpv, fop, \"s-\", label=f\"{best_model_name}\"); plt.plot([0,1],[0,1],\"k:\",label=\"Perfectly calibrated\")\n",
    "            plt.xlabel(\"Mean Predicted Probability\"); plt.ylabel(\"Fraction of Positives\"); plt.title(f'Calibration Plot - {best_model_name}')\n",
    "            plt.legend(); plt.grid(True, alpha=0.3)\n",
    "            calib_path = 'outputs_ml/calibration_plot.png'; plt.savefig(calib_path, dpi=300, bbox_inches='tight'); mlflow.log_artifact(calib_path); plt.close()\n",
    "        try:\n",
    "            print(\"Computing SHAP values...\")\n",
    "            explainer = shap.Explainer(best_model, X_test_use)\n",
    "            shap_values = explainer(X_test_use)\n",
    "            shap.summary_plot(shap_values, X_test_use, feature_names=feature_names, show=False)\n",
    "            plt.tight_layout(); shap_path = 'outputs_ml/shap_summary.png'; plt.savefig(shap_path, dpi=300, bbox_inches='tight'); mlflow.log_artifact(shap_path); plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"SHAP analysis skipped: {e}\")\n",
    "        if is_linear:\n",
    "            print(\"Validating linear model assumptions...\")\n",
    "            try:\n",
    "                y_pred = best_model.predict(X_test_use); residuals = self.y_test - y_pred\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                plt.subplot(2,2,1); plt.scatter(y_pred, residuals, alpha=0.6); plt.axhline(0, color='r', ls='--'); plt.title('Residuals vs Fitted'); plt.xlabel('Fitted'); plt.ylabel('Residuals')\n",
    "                plt.subplot(2,2,2); stats.probplot(residuals, dist=\"norm\", plot=plt); plt.title('Q-Q Plot')\n",
    "                plt.subplot(2,2,3); plt.hist(residuals, bins=20, density=True, alpha=0.7); plt.title('Residuals Distribution'); plt.xlabel('Residuals'); plt.ylabel('Density')\n",
    "                plt.subplot(2,2,4); plt.scatter(y_pred, np.sqrt(np.abs(residuals)), alpha=0.6); plt.title('Scale-Location'); plt.xlabel('Fitted'); plt.ylabel('‚àö|Residuals|')\n",
    "                plt.tight_layout(); lav_path = 'outputs_ml/linear_assumptions_validation.png'; plt.savefig(lav_path, dpi=300, bbox_inches='tight'); mlflow.log_artifact(lav_path); plt.close()\n",
    "                mlflow.log_metric(\"residuals_mean\", float(np.mean(residuals))); mlflow.log_metric(\"residuals_std\", float(np.std(residuals)))\n",
    "                dw_stat = durbin_watson(residuals); mlflow.log_metric(\"durbin_watson_stat\", float(dw_stat))\n",
    "                print(f\"    Residuals mean: {np.mean(residuals):.4f}\\n    Residuals std: {np.std(residuals):.4f}\\n    Durbin-Watson: {dw_stat:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Linear assumption validation skipped: {e}\")\n",
    "        print(\"Advanced analysis complete.\\n\")\n",
    "ModelDevelopmentPipeline.advanced_model_analysis = _mdp_advanced_model_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30088e20",
   "metadata": {},
   "source": [
    "## 4) Run Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "167fb5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/31 23:42:27 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/07/31 23:42:27 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow experiment: Orthopedic_Patients_Classification\n",
      "Experiment ID: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.ModelDevelopmentPipeline at 0x13ef170e0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Instantiate pipeline\n",
    "pipeline = ModelDevelopmentPipeline(cfg)\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b7f9fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data...\n",
      "Dataset shape: (310, 8)\n",
      "Target distribution:\n",
      "binary_class\n",
      "Abnormal    210\n",
      "Normal      100\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load data\n",
    "pipeline.load_and_prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7a94eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting and scaling data...\n",
      "Training set: (186, 6)\n",
      "Validation set: (62, 6)\n",
      "Test set: (62, 6)\n",
      "Checking linear model assumptions...\n",
      "  Checking multicollinearity...\n",
      "    Features with VIF > 5.0: 5\n",
      "  Checking feature normality...\n",
      "    Non-normal features: 6\n",
      "  Checking for outliers...\n",
      "    Total outliers detected: 45\n",
      "Applying data transformations...\n",
      "  Applying power transformation to degree_spondylolisthesis...\n",
      "    Power transformation applied (lambda: 0.5283)\n",
      "  Applying standard scaling to all columns...\n",
      "  Applying feature selection...\n",
      "    Selected features: ['pelvic_tilt', 'lumbar_lordosis_angle', 'pelvic_radius', 'degree_spondylolisthesis']\n",
      "Applying data transformations...\n",
      "  Applying power transformation to degree_spondylolisthesis...\n",
      "    Power transformation applied (lambda: 0.5283)\n",
      "  Applying standard scaling to all columns...\n",
      "Handling class imbalance using SMOTE...\n",
      "  Original distribution: {0: np.int64(126), 1: np.int64(60)}\n",
      "  New distribution: {0: np.int64(126), 1: np.int64(126)}\n",
      "Handling class imbalance using SMOTE...\n",
      "  Original distribution: {0: np.int64(126), 1: np.int64(60)}\n",
      "  New distribution: {0: np.int64(126), 1: np.int64(126)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split & scale\n",
    "pipeline.split_and_scale_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35029647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating models...\n",
      "\n",
      "==================================================\n",
      "Training LOGISTIC_REGRESSION\n",
      "==================================================\n",
      "Performing hyperparameter tuning...\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/31 23:42:30 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì logistic_regression training completed\n",
      "  Best validation F1: 0.7788\n",
      "  Test F1: 0.7815\n",
      "\n",
      "==================================================\n",
      "Training RANDOM_FOREST\n",
      "==================================================\n",
      "Performing hyperparameter tuning...\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/31 23:42:40 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì random_forest training completed\n",
      "  Best validation F1: 0.8184\n",
      "  Test F1: 0.8087\n",
      "\n",
      "==================================================\n",
      "Training GRADIENT_BOOSTING\n",
      "==================================================\n",
      "Performing hyperparameter tuning...\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/31 23:42:52 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì gradient_boosting training completed\n",
      "  Best validation F1: 0.8387\n",
      "  Test F1: 0.8237\n",
      "\n",
      "==================================================\n",
      "Training SVM\n",
      "==================================================\n",
      "Performing hyperparameter tuning...\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/31 23:42:57 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì svm training completed\n",
      "  Best validation F1: 0.8387\n",
      "  Test F1: 0.8437\n",
      "\n",
      "==================================================\n",
      "Training NAIVE_BAYES\n",
      "==================================================\n",
      "Performing hyperparameter tuning...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/31 23:42:59 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì naive_bayes training completed\n",
      "  Best validation F1: 0.7275\n",
      "  Test F1: 0.8124\n",
      "\n",
      "==================================================\n",
      "Training DECISION_TREE\n",
      "==================================================\n",
      "Performing hyperparameter tuning...\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/31 23:43:02 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì decision_tree training completed\n",
      "  Best validation F1: 0.8213\n",
      "  Test F1: 0.7938\n",
      "\n",
      "==================================================\n",
      "Training KNN\n",
      "==================================================\n",
      "Performing hyperparameter tuning...\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/31 23:43:04 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì knn training completed\n",
      "  Best validation F1: 0.7768\n",
      "  Test F1: 0.8277\n",
      "\n",
      "==================================================\n",
      "Training MLP\n",
      "==================================================\n",
      "Performing hyperparameter tuning...\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/31 23:43:11 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì mlp training completed\n",
      "  Best validation F1: 0.8184\n",
      "  Test F1: 0.8087\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train & evaluate models (with tuning)\n",
    "pipeline.train_and_evaluate_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34c6ae9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL COMPARISON AND SELECTION\n",
      "============================================================\n",
      "Model Performance Comparison:\n",
      "                 Model  Val_Accuracy  Val_Precision  Val_Recall  Val_F1  \\\n",
      "3                  svm        0.8387         0.8387      0.8387  0.8387   \n",
      "6                  knn        0.7742         0.7811      0.7742  0.7768   \n",
      "2    gradient_boosting        0.8387         0.8387      0.8387  0.8387   \n",
      "4          naive_bayes        0.7258         0.7296      0.7258  0.7275   \n",
      "1        random_forest        0.8226         0.8187      0.8226  0.8184   \n",
      "7                  mlp        0.8226         0.8187      0.8226  0.8184   \n",
      "5        decision_tree        0.8226         0.8206      0.8226  0.8213   \n",
      "0  logistic_regression        0.7742         0.7899      0.7742  0.7788   \n",
      "\n",
      "   Test_Accuracy  Test_Precision  Test_Recall  Test_F1  Test_ROC_AUC  \n",
      "3         0.8387          0.8764       0.8387   0.8437        0.9024  \n",
      "6         0.8226          0.8538       0.8226   0.8277        0.9494  \n",
      "2         0.8226          0.8252       0.8226   0.8237        0.9190  \n",
      "4         0.8065          0.8449       0.8065   0.8124        0.8833  \n",
      "1         0.8065          0.8127       0.8065   0.8087        0.9024  \n",
      "7         0.8065          0.8127       0.8065   0.8087        0.8940  \n",
      "5         0.7903          0.8009       0.7903   0.7938        0.8018  \n",
      "0         0.7742          0.8286       0.7742   0.7815        0.8762  \n",
      "\n",
      "üèÜ Best Model: svm (Test F1: 0.8437)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'svm'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Compare models\n",
    "best_model_name = pipeline.compare_models()\n",
    "best_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aae0cd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ADVANCED ANALYSIS - SVM\n",
      "============================================================\n",
      "Generating learning curves...\n",
      "Calculating permutation importance...\n",
      "Analyzing model calibration...\n",
      "Computing SHAP values...\n",
      "SHAP analysis skipped: The passed model is not callable and cannot be analyzed directly with the given masker! Model: SVC(C=100, gamma='auto', probability=True, random_state=42)\n",
      "Validating linear model assumptions...\n",
      "    Residuals mean: -0.1290\n",
      "    Residuals std: 0.3803\n",
      "    Durbin-Watson: 1.9000\n",
      "Advanced analysis complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Advanced analysis on best model\n",
    "pipeline.advanced_model_analysis(best_model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
